# Receipt Processing Pipeline

> Complete flow from upload to parsing with dynamic model selection

---

## Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Receipt Upload â†’ AI Parsing Flow                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  ğŸ–¥ï¸  FRONTEND (Tauri + React)          â˜ï¸  BACKEND (AWS Lambda + Bedrock) â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  1. Capture                              4. Process (Model Selection)      â”‚
â”‚  2. Compress & Dedupe                    5. AI Parse                       â”‚
â”‚  3. Upload to S3                         6. Multi-Model Compare (Optional) â”‚
â”‚                                          7. Save to DynamoDB              â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Part 1: Frontend Flow

### 1ï¸âƒ£ Capture & Compress

```
User drops receipt image
      â†“
captureService.handleDrop(file)
â”œâ”€ Create ReceiptImage { id, traceId, userId, localPath }
â”œâ”€ Save to SQLite (imageId, status='pending')
â”œâ”€ Update captureStore â†’ UI re-renders
â”‚
â””â”€ Start polling: processPendingImages() [1s intervals]
   â”‚
   â”œâ”€ fileService.processFile(image)
   â”‚  â”œâ”€ Tauri IPC: invoke('compress_image')
   â”‚  â”‚  Returns: { outputPath, md5, sizes }
   â”‚  â”‚
   â”‚  â”œâ”€ Check duplicate: findImageByMd5(md5)
   â”‚  â”‚  â”œâ”€ If duplicate â†’ status='skipped' âŒ
   â”‚  â”‚  â””â”€ If new â†’ status='compressed' âœ…
   â”‚  â”‚
   â”‚  â””â”€ Save to SQLite: md5, compressed_path, sizes
   â”‚
   â””â”€ Update captureStore { status='compressed', thumbnailPath }
```

**Files:**
- Frontend: `captureService.ts`, `fileService.ts`
- Store: `captureStore.ts`
- Adapter: `imageDb.ts`, `imageIpc.ts`

---

### 2ï¸âƒ£ Auto-Enqueue for Upload

```
captureStore state change
      â†“
Auto-trigger: enqueueCompressedImages()
â”œâ”€ Find all images with status='compressed'
â”œâ”€ Skip already-queued
â”‚
â””â”€ For each new image:
   uploadService.enqueue(id, filePath, traceId)
   â””â”€ uploadStore { status='idle' â†’ ready for polling }
```

**Files:**
- `captureService.ts:256-283` (enqueueCompressedImages)
- `uploadService.ts:79` (enqueue function)

---

### 3ï¸âƒ£ Upload to S3 with Rate Limiting

```
uploadService.startPolling() [1s intervals]
      â”œâ”€ Pre-conditions: online, quotaOK, noCurrentUpload
      â”‚
      â”œâ”€ fetchQuota(userId) â†’ AWS Lambda
      â”‚  â””â”€ Returns: { used, limit, remaining }
      â”‚
      â”œâ”€ canUpload(used, limit, lastTime)
      â”‚  â”œâ”€ Check quota: used >= limit â†’ pause('quota')
      â”‚  â”œâ”€ Check rate: within 2 seconds â†’ wait
      â”‚  â””â”€ Else â†’ allowed
      â”‚
      â””â”€ uploadService.processTask(id, filePath, traceId)
         â”œâ”€ uploadStore.startUpload() â†’ status='uploading'
         â”‚
         â”œâ”€ getPresignedUrl(userId, fileName, traceId)
         â”‚  â””â”€ AWS Lambda presign handler
         â”‚     â””â”€ Returns: { url, key, traceId }
         â”‚
         â”œâ”€ Read file: readFile(filePath) [Tauri fs]
         â”‚
         â”œâ”€ uploadToS3(presignedUrl, blob) [60s timeout]
         â”‚
         â”œâ”€ On SUCCESS:
         â”‚  â”œâ”€ fileService.updateStatus(id, 'uploaded') â†’ SQLite
         â”‚  â”œâ”€ uploadStore.uploadSuccess() â†’ UI refresh
         â”‚  â””â”€ Emit 'upload:complete', 'data:refresh' events
         â”‚
         â””â”€ On FAILURE:
            â”œâ”€ Classify error: network/server/quota/unknown
            â”œâ”€ uploadStore.uploadFailure()
            â””â”€ Schedule retry [1s, 2s, 4s backoff]
```

**Rate Limit Logic** (Domain: `receipt/index.ts`):
- Daily quota: 50 images/day (per user)
- Upload interval: 2 seconds minimum between uploads
- Backoff: exponential on network/server errors

**Files:**
- `uploadService.ts:90-238` (processQueue, processTask)
- `uploadStore.ts:107-200` (FSM states)
- `uploadApi.ts` (presign, S3 upload)
- `quotaApi.ts` (fetch quota)

---

## Part 2: Backend Flow (Lambda + Bedrock)

### 4ï¸âƒ£ Model Selection & Configuration

```
S3 ObjectCreated event
      â†“
instant-processor Lambda handler
      â”œâ”€ Parse S3 event: extract bucket, key, userId
      â”‚
      â”œâ”€ ğŸ”‘ LOAD CONFIG (DynamoDB CONTROL_TABLE)
      â”‚  â””â”€ GetItem { key: 'batch_config' }
      â”‚     â”œâ”€ primaryModelId (default: 'us.amazon.nova-lite-v1:0')
      â”‚     â”œâ”€ enableComparison (true/false)
      â”‚     â”œâ”€ comparisonModels: ['textract', 'nova_mini', 'nova_pro', 'azure_di']
      â”‚     â”œâ”€ azureConfig: { secretArn, endpoint }
      â”‚     â””â”€ mode: 'instant' | 'batch' | 'hybrid'
      â”‚
      â”œâ”€ ğŸ” RECOVER TRACE_ID (Pillar N)
      â”‚  â””â”€ S3 HeadObject â†’ read metadata['trace-id']
      â”‚
      â””â”€ Check mode: if mode !== 'instant', STOP
         (Batch/hybrid handled by different Lambda)
```

**Configuration Schema:**
```typescript
interface BatchConfig {
  key: 'batch_config';
  primaryModelId: string;           // 'azure_di' | 'us.amazon.nova-lite-v1:0' | ...
  enableComparison: boolean;
  comparisonModels?: string[];      // Optional comparison models
  azureConfig?: {
    secretArn: string;
    endpoint: string;
  };
  mode: 'instant' | 'batch' | 'hybrid';
  batchTime?: string;
  timezone?: string;
}
```

**Files:**
- `instant-processor/index.mjs:186-263` (config loading & model selection)
- `shared/schemas.mjs` (BatchConfigSchema validation)

---

### 5ï¸âƒ£ Primary Model Processing

```
â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶
â”‚  Branch A: Azure DI              Branch B: Bedrock (Nova)    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

If primaryModelId === 'azure_di':  Else (Bedrock):
  â”‚                                  â”‚
  â”œâ”€ Load Azure Credentials         â”œâ”€ modelId = primaryModelId
  â”‚  from Secrets Manager           â”‚  (e.g., 'us.amazon.nova-lite-v1:0')
  â”‚                                  â”‚
  â”œâ”€ analyzer.analyzeAzureDI(      â”œâ”€ Build OCR prompt:
  â”‚   imageBase64,                 â”‚  â”œâ”€ JSON schema instructions
  â”‚   azureCredentials              â”‚  â”œâ”€ Category list
  â”‚ )                               â”‚  â”œâ”€ Merchant list context
  â”‚ Returns: ModelResult           â”‚  â””â”€ Language: Japanese + English
  â”‚  â”œâ”€ amount                     â”‚
  â”‚  â”œâ”€ merchant                   â”œâ”€ bedrock.InvokeModelCommand({
  â”‚  â”œâ”€ category                   â”‚   modelId,
  â”‚  â”œâ”€ date                        â”‚   contentType: 'application/json',
  â”‚  â”œâ”€ description                â”‚   body: JSON.stringify(payload)
  â”‚  â”œâ”€ confidence (0-100)         â”‚ })
  â”‚  â””â”€ vendor: 'azure_di'         â”‚ Returns: response.body text
  â”‚                                 â”‚
  â””â”€ primaryModelId = 'azure_di'    â”œâ”€ Parse JSON response:
     primaryConfidence = 85          â”‚  â”œâ”€ Parse JSON.parse(text)
                                    â”‚  â”œâ”€ Strip markdown code blocks
                                    â”‚  â””â”€ Extract fields
                                    â”‚
                                    â””â”€ primaryModelId = modelId
                                       primaryConfidence = undefined
                                       (Nova doesn't return confidence)
```

**Model Details:**

| Model | Provider | Confidence | Cost | Speed |
|-------|----------|-----------|------|-------|
| **nova-lite** | AWS Bedrock | âŒ None | Base | Fast |
| **nova-mini** | AWS Bedrock | âŒ None | ~0.5x | Faster |
| **nova-pro** | AWS Bedrock | âŒ None | ~2x | Slower |
| **textract** | AWS Textract | âŒ None | ~1x | Medium |
| **azure_di** | Microsoft Azure | âœ… 0-100 | ~1x | Medium |

**Files:**
- `instant-processor/index.mjs:269-349` (primary model execution)
- `shared/model-analyzer.mjs` (MultiModelAnalyzer)
  - `analyzeAzureDI()` - Azure Document Intelligence
  - `analyzeBedrock()` - Bedrock Nova models
  - `convertModelResultToOcrResult()` - Format conversion

---

### 6ï¸âƒ£ Validation (Airlock - Pillar B)

```
Primary model output
      â†“
Parse JSON response (strip markdown)
      â†“
OcrResultSchema.parse(json)
â”œâ”€ amount: number
â”œâ”€ type: 'income' | 'expense'
â”œâ”€ date: YYYY-MM-DD
â”œâ”€ merchant: string
â”œâ”€ category: enum
â”œâ”€ description: string
â”‚
â””â”€ On validation failure:
   â”œâ”€ Log AIRLOCK_BREACH event
   â””â”€ Skip this record, continue next
```

**Files:**
- `shared/schemas.mjs` (OcrResultSchema)
- `instant-processor/index.mjs:341-349` (validation)

---

### 7ï¸âƒ£ Multi-Model Comparison (Optional - Pillar R)

```
If enableComparison === true:
      â”œâ”€ Parallel execute enabled models:
      â”‚  â”œâ”€ nova-lite
      â”‚  â”œâ”€ nova-mini
      â”‚  â”œâ”€ nova-pro
      â”‚  â””â”€ azure_di (if credentials available)
      â”‚
      â”œâ”€ analyzer.analyzeReceipt({
      â”‚   imageBase64,
      â”‚   enabledModels,
      â”‚   azureCredentials
      â”‚ })
      â”‚ Returns: ModelComparison {
      â”‚   nova_mini: {...},
      â”‚   nova_pro: {...},
      â”‚   azure_di: {...},
      â”‚   textract: {...}
      â”‚ }
      â”‚
      â””â”€ Non-blocking: failures don't block primary result
         (continues with primary model result if comparison fails)
```

**Data Structure:**
```typescript
interface ModelComparison {
  [modelName: string]: {
    amount: number;
    merchant: string;
    category: string;
    date: string;
    confidence?: number;
    processingTime?: number;
    error?: string;
  }
}
```

**Files:**
- `instant-processor/index.mjs:351-373` (multi-model comparison)
- `shared/model-analyzer.mjs:analyzeReceipt()` (parallel execution)

---

### 8ï¸âƒ£ Save to DynamoDB

```
Build Transaction object:
â”œâ”€ From primary model result:
â”‚  â”œâ”€ amount, merchant, category, date, description
â”‚  â””â”€ primaryModelId (e.g., 'azure_di')
â”‚  â””â”€ primaryConfidence (if Azure, else null)
â”‚
â”œâ”€ Metadata:
â”‚  â”œâ”€ userId, imageId, transactionId
â”‚  â”œâ”€ s3Key (moved from uploads/ â†’ processed/)
â”‚  â”œâ”€ status: 'unconfirmed' (user confirms later)
â”‚  â”œâ”€ createdAt, updatedAt (ISO 8601 UTC)
â”‚  â””â”€ traceId (Pillar N: distributed tracing)
â”‚
â””â”€ Optional:
   â””â”€ modelComparison (if enabled)

Put to DynamoDB:
â”œâ”€ Table: TRANSACTIONS_TABLE_NAME
â”œâ”€ Key: { userId, transactionId }
â”œâ”€ TTL: 60 days (if guest user)
â””â”€ Success: Transaction ready for sync

Move S3 image:
â”œâ”€ CopyObject: uploads/{imageId} â†’ processed/{imageId}
â””â”€ (Optional: delete uploads/{imageId} after TTL)
```

**Transaction Record:**
```typescript
interface Transaction {
  userId: string;
  transactionId: string;
  imageId: string;
  amount: number;
  type: 'income' | 'expense';
  date: string;           // YYYY-MM-DD
  merchant: string;
  category: string;
  description: string;
  status: 'unconfirmed' | 'confirmed' | 'deleted' | 'needs_review';

  // Model tracking
  primaryModelId: string;         // 'nova-lite', 'azure_di', etc.
  primaryConfidence?: number;     // 0-100 (if Azure)
  modelComparison?: ModelComparison;  // Optional

  // Metadata
  s3Key: string;                  // processed/{imageId}
  createdAt: string;              // ISO 8601 UTC
  updatedAt: string;
  confirmedAt?: string;
  traceId: string;                // Pillar N: log correlation
  version: number;
  ttl?: number;                   // Unix timestamp (guest only)
}
```

**Files:**
- `instant-processor/index.mjs:375-450` (transaction creation & DynamoDB put)
- `shared/schemas.mjs` (TransactionSchema)

---

## Part 3: Result Display

### 9ï¸âƒ£ Fetch & Sync Results

```
User navigates to Transaction/Ledger View
      â†“
useTransactionLogic(userId) hook
â”œâ”€ Listen to 'data:refresh' event (from upload complete)
â”‚
â”œâ”€ transactionService.loadTransactions(userId)
â”‚  â”œâ”€ Priority 1: Local SQLite cache
â”‚  â”œâ”€ Priority 2: Cloud Lambda (10s timeout)
â”‚  â”‚  â””â”€ POST VITE_LAMBDA_SYNC_URL
â”‚  â”‚     Returns: { transactions: [...], cursor }
â”‚  â”‚     Validation: FetchTransactionsResponseSchema (Zod)
â”‚  â”‚
â”‚  â””â”€ Priority 3: Cache in SQLite for next load
â”‚
â””â”€ Transform: mapCloudToTransaction()
   â””â”€ Add timestamps, process IDs
```

### ğŸ”Ÿ Display Transaction

```
React: TransactionView.tsx
â”œâ”€ status === 'loading' â†’ Show skeleton
â”œâ”€ status === 'error' â†’ Show error + retry
â””â”€ status === 'idle' â†’ Display table:
   â”œâ”€ Date
   â”œâ”€ Merchant (AI-extracted)
   â”œâ”€ Category (with icon)
   â”œâ”€ Amount (formatted)
   â”œâ”€ Status (unconfirmed/confirmed badge)
   â””â”€ Confidence (if available, e.g., "92%" for Azure)
      â””â”€ On hover: Show primaryModelId (e.g., "Processed by Azure DI")

User actions:
â”œâ”€ Click row â†’ Edit transaction
â”œâ”€ Approve â†’ status='confirmed' â†’ Save to cloud
â””â”€ Delete â†’ status='deleted' â†’ Sync on next uplink
```

**Files:**
- `TransactionView.tsx` (UI component)
- `useTransactionLogic.ts` (headless hook)
- `transactionService.ts` (orchestration)
- `transactionApi.ts` (cloud fetch)

---

## State Machines

### Image Lifecycle (Pillar D: FSM)

```
pending
  â†“ (compression starts)
compressing
  â”œâ”€ (MD5 duplicate found) â†’ skipped âŒ
  â””â”€ (new image) â†’ compressed âœ…
      â†“ (auto-enqueue)
      uploading
        â”œâ”€ (success) â†’ uploaded âœ…
        â”‚   â””â”€ (Lambda processes) â†’ transaction created
        â””â”€ (failure) â†’ retrying
            â”œâ”€ (backoff expires) â†’ idle â†’ uploading (retry)
            â””â”€ (max retries exceeded) â†’ failed âŒ
```

### Upload Queue FSM

```
idle â”€â”€â†’ processing â”€â”€â†’ paused (offline/quota)
  â†‘                          â†“
  â””â”€â”€ resume â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Task Status (per item):
idle â†’ uploading â†’ (success: idle) | (retry: retrying) | (failed: failed)
```

---

## Configuration Management

### Reading Config (At Lambda Start)

```
Lambda cold start
      â†“
Load merchant list (cached in memory)
      â†“
For each S3 record:
  â”œâ”€ Recover traceId from S3 metadata
  â”œâ”€ DynamoDB GetItem { key: 'batch_config' }
  â””â”€ Apply config:
     â”œâ”€ Select primaryModelId
     â”œâ”€ Check enableComparison
     â””â”€ Load Azure credentials if needed
```

### Admin Configuration (Frontend)

```
Admin Panel â†’ Processing Settings
â”œâ”€ Primary Model dropdown:
â”‚  â”œâ”€ nova-lite (default, fastest, no confidence)
â”‚  â”œâ”€ nova-mini
â”‚  â”œâ”€ nova-pro
â”‚  â””â”€ azure_di (includes confidence score)
â”‚
â”œâ”€ Enable Model Comparison (checkbox)
â”‚  â””â”€ If enabled: select which models to compare
â”‚
â”œâ”€ Processing Mode (radio):
â”‚  â”œâ”€ Instant (1 image â†’ full price)
â”‚  â”œâ”€ Batch (100+ images â†’ 50% off)
â”‚  â””â”€ Hybrid (mixed)
â”‚
â”œâ”€ Batch Settings:
â”‚  â”œâ”€ Image Threshold (100-500, default 100)
â”‚  â””â”€ Timeout Minutes (30-480)
â”‚
â””â”€ Save â†’ Update DynamoDB batch_config

Effects:
â”œâ”€ Instant processing uses new model immediately
â”œâ”€ Batch processing applies to next batch job
â””â”€ Comparison results appear in transaction.modelComparison
```

**Files:**
- Admin Panel component (to be created)
- `admin-config-save` Lambda (to be created)
- Admin authorization (via Cognito)

---

## Timeout Values & Error Handling

| Operation | Timeout | Error Type | Retry? |
|-----------|---------|-----------|--------|
| Compression (Tauri) | 15s | timeout | Max 3x |
| Presign URL | 10s | network | Exponential backoff |
| S3 upload | 60s | network/server | Exponential backoff |
| Transaction fetch | 10s | network | Fallback to cache |
| Bedrock API | SDK default | varies | Non-blocking on comparison |
| Azure DI | 30s | network | Fail entire record |

**Backoff Strategy (Upload Retry):**
```
Delay 1: 1000ms
Delay 2: 2000ms
Delay 3: 4000ms
Max retries: 3

Error Classification:
- network/timeout â†’ RETRY
- server (5xx) â†’ RETRY
- quota (429, 403) â†’ NO RETRY
- unknown â†’ NO RETRY
```

---

## Distributed Tracing (Pillar N)

```
Frontend creates traceId:
â”œâ”€ `createTraceId()` at image drop
â””â”€ Store in Image record

Frontend â†’ S3 Upload:
â”œâ”€ Include in presigned URL metadata
â””â”€ S3 stores as `x-amz-meta-trace-id`

S3 â†’ Lambda:
â”œâ”€ Lambda recovers: `recoverTraceIdFromS3()`
â”œâ”€ Uses for all subsequent logs
â””â”€ All database records include traceId

Logging:
â”œâ”€ Local SQLite: images.trace_id, transactions.traceId
â”œâ”€ CloudWatch: all Lambda logs include traceId
â””â”€ Query by traceId: trace complete request chain
```

**Files:**
- `instant-processor/index.mjs:68-92` (trace recovery)
- `shared/logger.mjs` (include traceId in all events)

---

## Related Documentation

| Document | Purpose |
|----------|---------|
| [FLOWS.md](./FLOWS.md) | High-level system flows |
| [SCHEMA.md](./SCHEMA.md) | Database & API schemas |
| [LAYERS.md](./LAYERS.md) | System architecture layers |
| [ADR-014](./ADR/ADR-014-three-tier-lambda-deployment.md) | Three-tier Lambda deployment |
| [INTERFACES.md](./INTERFACES.md) | IPC commands & API endpoints |

---

**Last Updated:** 2026-01-18
**Version:** 1.0
**Status:** Complete flow with dynamic model selection
